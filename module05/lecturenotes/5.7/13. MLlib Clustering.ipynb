{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Clustering\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "### Last Updated: Feb 26, 2020\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES  \n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "\n",
    "- https://spark.apache.org/docs/latest/mllib-clustering.html  \n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "Introduction to some of the major clustering techniques in MLlib  \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Unsupervised learning\n",
    "- K-means\n",
    "- Mixture of Gaussians\n",
    "\n",
    "---\n",
    "\n",
    "**Unsupervised Learning**  \n",
    "In this task, labels are unknown and the analyst wishes to segment the observations into groups of high similarity, where similarity is defined in terms of the feature space.\n",
    "\n",
    "Common use cases are:\n",
    "- Data exploration to discover the properties of similar observations  \n",
    "- Outlier detection; outliers will generally form their own group (e.g., singletons)  \n",
    "\n",
    "**K-Means**  \n",
    "This is the most popular clustering algorithm, with widespread use in industry. It is relatively simple, uses a single parameter, and converges on a solution (but possibly not the global maximum).\n",
    "\n",
    "The following models are supported in `spark.mllib`:\n",
    "\n",
    "- K-means\n",
    "- Gaussian mixture\n",
    "- Power iteration clustering (PIC)\n",
    "- Latent Dirichlet allocation (LDA)\n",
    "- Bisecting k-means\n",
    "- Streaming k-means\n",
    "\n",
    "**<center>K-Means Specs</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Item   | Description |\n",
    "| -------- | ----------- |\n",
    "| Supervised/Unsupervised | Unsupervised |\n",
    "| Initialization | Random Assignment |\n",
    "| Assumptions | Euclidean Distance |\n",
    "| Preprocessing | Scaling |\n",
    "| Parameters | $K:$ number of clusters |\n",
    "| Metrics | Inertia |\n",
    "| Strengths | One parameter, relatively simple |\n",
    "| Weaknesses | 1. May not find global optimum <br> 2. Can't handle non-quant data (e.g., categorical)<br> 3. Assumes spherical cluster shape|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Sample 2D Visualization ($K=3$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k_means_before_after.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-Means Sample Workflow | \n",
    "| -------- | \n",
    "| 1. feature selection | \n",
    "| 2. feature standardization | \n",
    "| 3. run algo for sequence of $K$ |  \n",
    "| 4. examine results and remediate outliers <br> <span style=\"color:red\">loop on 3-4 as needed</span>| \n",
    "| 5. select $K^*$, extract labels | \n",
    "| 6. enrich with domain knowledge | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means: Selecting $K^*$**\n",
    "\n",
    "One method for selecting $K^*$ is by identifying the elbow in a scree plot.  At the inflection point, adding more clusters reduces WSS only marginally.  Generally, well-formed clusters are split apart, creating new ones.\n",
    "\n",
    "$$ Within Sum of Squares (WSS) = 1 - \\frac{Between Sum of Squares}{Total Sum of Squares} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='scree_plot_k_means2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Implementation**\n",
    "\n",
    "`MLlib` contains an implementation of `K-means` and also `K-means||`  \n",
    "`K-means||` provides a better initialization in parallel environments.  \n",
    "\n",
    "Included in the parameters,  \n",
    "`initializationMode` specifies either `random` initialization or initialization via `k-means||`.\n",
    "\n",
    "`K-means` takes an RDD of `Vectors`  \n",
    "\n",
    "**Methods:**  \n",
    "Can access `clusterCenters` as an array of vectors  \n",
    "Can call `predict()` on a new vector to return its assigned cluster;   this is the closest center.\n",
    "\n",
    "**K-Means Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sum([x**2 for x in (point - center)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "clusters.save(sc, \"KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting other clustering models requires calling the appropriate training function.  \n",
    "\n",
    "**Gaussian Mixture Model**    \n",
    "The *Gaussian Mixture Model* is a weighted combination of underlying Gaussian distributions, each with a fixed probability.  The *expectation-maximization algorithm* is used in `spark.mllib` to estimate the parameters.  \n",
    "\n",
    "**Mixture of Gaussians, Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for illustration only, does not run\n",
    "\n",
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "gmm = GaussianMixture.train(parsedData, 2)\n",
    "\n",
    "\n",
    "# output parameters of model\n",
    "for i in range(2):\n",
    "    print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "          \"sigma = \", gmm.gaussians[i].sigma.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **K-Means: Loop over *maxIterations***  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Remove the \"save and load model\" parts of the code  \n",
    "iii. Modify the code to loop over *maxIterations* for values 10, 20, 30 and print the WSSSE  \n",
    "iv. Run the results and note how WSSSE varies as a function of *maxIterations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **K-Means: Loop over *K***  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Remove the \"save and load model\" parts of the code  \n",
    "iii. Modify the code to loop over the number of clusters *K* for values 2, 3, 4, 5 and print the WSSSE  \n",
    "iv. Run the results and note how WSSSE varies as a function of *K*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
