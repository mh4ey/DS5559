{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib - Basic Statistics\n",
    "\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "### Last Updated: Dec 12, 2019\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCE:\n",
    "\n",
    "1. Learning Spark  \n",
    "2. Spark Documentation  \n",
    "https://spark.apache.org/docs/latest/mllib-statistics.html\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.random.RandomRDDs\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to basic statistics capabilities  \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Summary statistics\n",
    "- Correlations\n",
    "- Stratified sampling\n",
    "- Hypothesis testing\n",
    "- Random data generation\n",
    "- Kernel density estimation\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we provide a short summary of the basic statistics functionality provided for RDDs.  Much of this is supported by the `pyspark.mllib.stat` library.  \n",
    "\n",
    "**Summary Statistics**  \n",
    "\n",
    "`colStats()`  \n",
    "Computes summary of an RDD of vectors. Similar to calling `summary()` on dataframe in R.\n",
    "\n",
    "#### Summary using `colStats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m, in \u001b[0;32mrun_code\u001b[0m:\nLine \u001b[0;34m3427\u001b[0m:  exec(code_obj, \u001b[36mself\u001b[39;49;00m.user_global_ns, \u001b[36mself\u001b[39;49;00m.user_ns)\n",
      "In  \u001b[0;34m[2]\u001b[0m:\nLine \u001b[0;34m6\u001b[0m:     spark = SparkSession.builder \\\n",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m, in \u001b[0;32mgetOrCreate\u001b[0m:\nLine \u001b[0;34m228\u001b[0m:   sc = SparkContext.getOrCreate(sparkConf)\n",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\pyspark\\context.py\u001b[0m, in \u001b[0;32mgetOrCreate\u001b[0m:\nLine \u001b[0;34m384\u001b[0m:   SparkContext(conf=conf \u001b[35mor\u001b[39;49;00m SparkConf())\n",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\pyspark\\context.py\u001b[0m, in \u001b[0;32m__init__\u001b[0m:\nLine \u001b[0;34m144\u001b[0m:   SparkContext._ensure_initialized(\u001b[36mself\u001b[39;49;00m, gateway=gateway, conf=conf)\n",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\pyspark\\context.py\u001b[0m, in \u001b[0;32m_ensure_initialized\u001b[0m:\nLine \u001b[0;34m331\u001b[0m:   SparkContext._gateway = gateway \u001b[35mor\u001b[39;49;00m launch_gateway(conf)\n",
      "File \u001b[0;34mC:\\Users\\mhomb\\miniconda3\\envs\\xeus-python\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m, in \u001b[0;32mlaunch_gateway\u001b[0m:\nLine \u001b[0;34m108\u001b[0m:   \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mJava gateway process exited before sending its port number\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.143.158.50:7077\") \\\n",
    "    .appName(\"mllib_classifier\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]),\n",
    "                      Vectors.dense([4, 5, 0,  3]),\n",
    "                      Vectors.dense([6, 7, 0,  8])])\n",
    "\n",
    "cStats = Statistics.colStats(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStats.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation  \n",
    "Currently supports Pearson, Spearman correlation  \n",
    "Takes parameter method. Set to one of ‘pearson’ (default) or ‘spearman’  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n",
    "\n",
    "Statistics.corr(seriesX,seriesY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Sampling\n",
    "\n",
    "For stratified sampling, the keys can be thought of as a label and the value as a specific attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an RDD of any key value pairs\n",
    "data = sc.parallelize([(1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')])\n",
    "\n",
    "# specify the exact fraction desired from each key as a dictionary\n",
    "fractions = {1: 0.1, 2: 0.6, 3: 0.3}\n",
    "\n",
    "approxSample = data.sampleByKey(False, fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis testing\n",
    "\n",
    "- Pearson’s Independence Test\n",
    "\n",
    "This function computes the goodness of fit. If a second vector to test against is not supplied as a parameter, the test runs against a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodnessOfFitTestResult = Statistics.chiSqTest(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pearson’s Goodness of Fit (GoF) Test \n",
    "\n",
    "This function conducts Pearson's independence test on the input contingency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independenceTestResult = Statistics.chiSqTest(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kolmogorov-Smirnov Test  \n",
    "\n",
    "`spark.mllib` provides a 1-sample, 2-sided implementation of the Kolmogorov-Smirnov (KS) test for equality of probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random data generation\n",
    "\n",
    "`spark.mllib` supports generating random RDDs with i.i.d. values drawn from a given distribution: \n",
    "- uniform\n",
    "- standard normal\n",
    "- Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomRDDs.normalRDD(sc, 1000000L, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density estimation\n",
    "\n",
    "KDE is a technique useful for visualizing empirical probability distributions without requiring assumptions about the particular distribution that the observed samples are drawn from. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "kd = KernelDensity()\n",
    "kd.setSample(data)\n",
    "kd.setBandwidth(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create an RDD containing vectors with 3 columns.  Using `colStats()`, compute the mean and number of nonzero values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute the Spearman correlation between the vectors from (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Generate 10,000 independent samples from a Poisson distribution with mean 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
