{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "### K-Means Cluster Analysis of Fidelity Fund Returns \n",
    "### Last updated: June 17, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this assignment, you will conduct a k-means cluster analysis on a set of Fidelity mutual funds.  The data consists of the daily returns (percentage change in price), with tickers as the identifiers.  The outline below will walk you through the required steps.  \n",
    "\n",
    "This assignment is worth a total of 10 POINTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "### (you can do these steps using pandas dataframes if that's easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset\n",
    "df0 = pd.read_csv('fido_returns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the tickers in a list called tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the index column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the data, putting funds on rows and timepoints on columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# print the shape of the transposed dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataframe w transposed data, calling it dft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data to file (in case you wish to work with it later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed data as RDD, calling it fido. Dates along columns, funds along rows\n",
    "fido = sc.textFile(\"fido_returns_funds_on_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the header row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# build RDD without header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# extract data by parsing out commas. cast to array of floats. hint: a list comprehension may be helpful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the K-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# parameters: use 3 clusters, and a maximum of 10 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# train k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 2 POINTS\n",
    "# Write a function called error() which takes a point as input and returns Within Set Sum of Squared Errors (WSSE)\n",
    "# None is a placeholder\n",
    "\n",
    "def error(point):\n",
    "    None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# Compute the WSSE by calling error()\n",
    "# Hint: you may want to include the error() function within a map() over the data, and then reduce() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 2 POINTS\n",
    "# Write code to fit k-means with k ranging from 2 through 10, inclusive.  Then print WSSE for each value of k.\n",
    "\n",
    "# range of k values\n",
    "num_clusters_list = None \n",
    "# set up storage\n",
    "kmeans_out = pd.DataFrame(columns=['wsse'], index=num_clusters_list)\n",
    "\n",
    "# here you will add the training, WSSE computation, WSSE storage, and final printing of kmeans_out\n",
    "for k in None:\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# insert code to plot k on the x-axis, and WSSE on the y-axis\n",
    "plt.xlabel('number_of_clusters')\n",
    "plt.ylabel('WSSE')\n",
    "plt.title('WSSE vs Clusters')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"results/fido_kmeans.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at predicted cluster membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE: 1 POINT\n",
    "# For the case k=10: \n",
    "# predict the cluster each fund belongs to\n",
    "# print the number of funds assigned to each cluster\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5559 Spark3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
