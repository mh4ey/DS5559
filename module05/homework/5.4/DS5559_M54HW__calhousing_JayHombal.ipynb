{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "### Linear Regression Modeling of California Home Prices\n",
    "### Last updated: Oct 21, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Jay Hombal\n",
    "### Computing Id: mh4ey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOTAL POINTS: 10**\n",
    "\n",
    "**Instructions**  \n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Please do the following:  \n",
    "\n",
    "1) (6 PTS) Go through all code and fill in the missing cells. This will prep data, train a model, predict, and evaluate model fit.  Compute and report the Mean Squared Error (MSE).  \n",
    "2) (1 PT) Repeat Part 1 with at least one additional feature from the original set.  \n",
    "3) (2 PTS) Repeat Part 1 with at least one engineered feature based on one or more variables from the original set.  \n",
    "4) (1 PT) Repeat Part 1 using Lasso Regression\n",
    "\n",
    "Please report resuts in the following way:  \n",
    "In the **RESULTS SECTION** table at the very bottom, there are three cells where you should copy your code from parts 2,3,4.  \n",
    "In the very last cell, print a dataframe containing two columns: `question_part` and `MSE`.  \n",
    "This dataframe must report your MSE results.\n",
    "\n",
    "**Data Source**  \n",
    "StatLib---Datasets Archive  \n",
    "http://lib.stat.cmu.edu/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:11.064394Z",
     "start_time": "2021-03-19T01:01:10.904391Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:15.674995Z",
     "start_time": "2021-03-19T01:01:11.067224Z"
    }
   },
   "outputs": [],
   "source": [
    "# read text file into pyspark dataframe\n",
    "filename = 'cal_housing_data_preproc_w_header.txt'\n",
    "df = spark.read.csv(filename,  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:16.070750Z",
     "start_time": "2021-03-19T01:01:15.675903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|    median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|           8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|           8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|          352100.0|7.257399999999999|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "|          341300.0|           5.6431|              52.0|     1274.0|         235.0|     558.0|     219.0|   37.85|  -122.25|\n",
      "|          342200.0|           3.8462|              52.0|     1627.0|         280.0|     565.0|     259.0|   37.85|  -122.25|\n",
      "|          269700.0|           4.0368|              52.0|      919.0|         213.0|     413.0|     193.0|   37.85|  -122.25|\n",
      "|          299200.0|           3.6591|              52.0|     2535.0|         489.0|    1094.0|     514.0|   37.84|  -122.25|\n",
      "|          241400.0|             3.12|              52.0|     3104.0|         687.0|    1157.0|     647.0|   37.84|  -122.25|\n",
      "|          226700.0|           2.0804|              42.0|     2555.0|         665.0|    1206.0|     595.0|   37.84|  -122.26|\n",
      "|          261100.0|           3.6912|              52.0|     3549.0|         707.0|    1551.0|     714.0|   37.84|  -122.25|\n",
      "|          281500.0|           3.2031|              52.0|     2202.0|         434.0|     910.0|     402.0|   37.85|  -122.26|\n",
      "|          241800.0|           3.2705|              52.0|     3503.0|         752.0|    1504.0|     734.0|   37.85|  -122.26|\n",
      "|          213500.0|            3.075|              52.0|     2491.0|         474.0|    1098.0|     468.0|   37.85|  -122.26|\n",
      "|          191300.0|           2.6736|              52.0|      696.0|         191.0|     345.0|     174.0|   37.84|  -122.26|\n",
      "|          159200.0|           1.9167|              52.0|     2643.0|         626.0|    1212.0|     620.0|   37.85|  -122.26|\n",
      "|          140000.0|            2.125|              50.0|     1120.0|         283.0|     697.0|     264.0|   37.85|  -122.26|\n",
      "|          152500.0|            2.775|              52.0|     1966.0|         347.0|     793.0|     331.0|   37.85|  -122.27|\n",
      "|          155500.0|           2.1202|              52.0|     1228.0|         293.0|     648.0|     303.0|   37.85|  -122.27|\n",
      "|          158700.0|           1.9911|              50.0|     2239.0|         455.0|     990.0|     419.0|   37.84|  -122.26|\n",
      "|          162900.0|           2.6033|              52.0|     1503.0|         298.0|     690.0|     275.0|   37.84|  -122.27|\n",
      "+------------------+-----------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Preprocessing\n",
    "\n",
    "We want to do three more things before training a model:  \n",
    "\n",
    "**SCALING (1 POINT)**   \n",
    "Scale the response variable median_house_value, dividing by 100000 and saving into column median_house_value_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:16.133013Z",
     "start_time": "2021-03-19T01:01:16.071468Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('median_house_value_final', col('median_house_value')/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE ENGINEERING**  **(1 POINT)**  \n",
    "Add new feature:  rooms_per_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:16.400760Z",
     "start_time": "2021-03-19T01:01:16.133830Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+\n",
      "|median_house_value_final|rooms_per_household|\n",
      "+------------------------+-------------------+\n",
      "|                   4.526|  6.984126984126984|\n",
      "|                   3.585|  6.238137082601054|\n",
      "|                   3.521|  8.288135593220339|\n",
      "|                   3.413| 5.8173515981735155|\n",
      "|                   3.422|  6.281853281853282|\n",
      "+------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('rooms_per_household', col('total_rooms')/col('households'))\n",
    "\n",
    "df.select('median_house_value_final','rooms_per_household').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELECT AND STANDARDIZE FEATURES**  **(2 POINTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:16.431516Z",
     "start_time": "2021-03-19T01:01:16.401500Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# retain these predictors for Part 1\n",
    "vars_to_keep = [\"median_house_value_final\", \n",
    "              \"total_bedrooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"median_income\", \n",
    "              \"rooms_per_household\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df1 = df.select(vars_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to standardize the features, but not the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:18.042994Z",
     "start_time": "2021-03-19T01:01:16.432749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "transformed_data = df1.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "transformed_df = spark.createDataFrame(transformed_data, ['label', 'features'])\n",
    "transformed_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:19.775606Z",
     "start_time": "2021-03-19T01:01:18.043897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "# use StandardScaler to scale the features to std normal distribution\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", \n",
    "                                withStd=True, withMean=False)\n",
    "\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler = standardScaler.fit(transformed_df)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df = scaler.transform(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train set (80%), test set (20%) using seed=314  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:19.802485Z",
     "start_time": "2021-03-19T01:01:19.776375Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 314\n",
    "train_test = [0.8, 0.2]\n",
    "train_data, test_data = scaled_df.randomSplit(train_test,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:19.811780Z",
     "start_time": "2021-03-19T01:01:19.803244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is [('label', 'double'), ('features', 'vector'), ('features_scaled', 'vector')]\n",
      "test data is a <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f'train data is {train_data.dtypes}')\n",
    "print(f'test data is a {type(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the linear regression object with given parameters **(1 POINT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:19.862610Z",
     "start_time": "2021-03-19T01:01:19.814053Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression # note this is from the ML package\n",
    "\n",
    "maxIter=10\n",
    "regParam=0.3\n",
    "elasticNetParam=0.8\n",
    "\n",
    "lr = LinearRegression (labelCol='label',\\\n",
    "                       maxIter=maxIter,\\\n",
    "                       elasticNetParam=elasticNetParam,\\\n",
    "                       regParam=regParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.082926Z",
     "start_time": "2021-03-19T01:01:19.863359Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.101115Z",
     "start_time": "2021-03-19T01:01:24.085882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bedrooms', 0.0),\n",
       " ('population', 0.0),\n",
       " ('households', 0.0),\n",
       " ('median_income', 0.2768150738137753),\n",
       " ('rooms_per_household', 0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(df1.columns[1:],linear_model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.106973Z",
     "start_time": "2021-03-19T01:01:24.103381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0006902251741692"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.123189Z",
     "start_time": "2021-03-19T01:01:24.109200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.summary.numInstances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each datapoint in the test set, make a prediction (hint: apply `transform()` to the model).\n",
    "You will want the returned object to be a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.159875Z",
     "start_time": "2021-03-19T01:01:24.125488Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'features_scaled', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "predictions = linear_model.transform(test_data)\n",
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:24.895937Z",
     "start_time": "2021-03-19T01:01:24.160606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+\n",
      "|  label|            features|     features_scaled|        prediction|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|0.14999|[267.0,628.0,225....|[0.63383104398397...|2.1614311926900918|\n",
      "|  0.225|[73.0,216.0,63.0,...|[0.17329463000310...| 1.741170547626018|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:25.520465Z",
     "start_time": "2021-03-19T01:01:24.896648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=2.1614311926900918, label=0.14999),\n",
       " Row(prediction=1.741170547626018, label=0.225)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsandlabels_df = predictions.select(\"prediction\", \"label\")\n",
    "predsandlabels_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPUTE MSE (1 POINT)**  \n",
    "Evaluate the model by computing Mean Squared Error (MSE), which is the average sum of squared differences between predicted and label. \n",
    "\n",
    "This can be computed in a single line using `reduce()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:26.755275Z",
     "start_time": "2021-03-19T01:01:25.522678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755384454564553"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE1 = predsandlabels_df\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: (x[0] - x[1])**2)\\\n",
    "    .reduce(lambda x,y : x+y) /predsandlabels_df.count()\n",
    "MSE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:27.457847Z",
     "start_time": "2021-03-19T01:01:26.755972Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691285604354243"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_eval = RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
    "lr_eval.evaluate(predsandlabels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:27.967585Z",
     "start_time": "2021-03-19T01:01:27.458562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713569550594953"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE\n",
    "prediction_mae = lr_eval.evaluate(predsandlabels_df, \n",
    "                                           {lr_eval.metricName:'mae'}) \n",
    "\n",
    "prediction_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:28.516232Z",
     "start_time": "2021-03-19T01:01:27.971011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755384454564553"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE\n",
    "prediction_mse = lr_eval.evaluate(predsandlabels_df, \n",
    "                                           {lr_eval.metricName:'mse'})\n",
    "\n",
    "prediction_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:29.099993Z",
     "start_time": "2021-03-19T01:01:28.518717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691285604354243"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE\n",
    "prediction_rmse = lr_eval.evaluate(predsandlabels_df, \n",
    "                                           {lr_eval.metricName:'rmse'}) \n",
    "\n",
    "prediction_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS SECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:29.275298Z",
     "start_time": "2021-03-19T01:01:29.100718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------+\n",
      "|median_house_value_final|total_bedrooms|population|households|median_income|rooms_per_household|housing_median_age|\n",
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------+\n",
      "|                   4.526|         129.0|     322.0|     126.0|       8.3252|  6.984126984126984|              41.0|\n",
      "|                   3.585|        1106.0|    2401.0|    1138.0|       8.3014|  6.238137082601054|              21.0|\n",
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code for Part 2 - (1 PT) Repeat Part 1 with at least one additional feature from the original set.\n",
    "# retain these predictors for Part 1\n",
    "vars_to_keep2 = [\"median_house_value_final\", \n",
    "              \"total_bedrooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"median_income\",\n",
    "              \"rooms_per_household\",\n",
    "              \"housing_median_age\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df2 = df.select(vars_to_keep2)\n",
    "\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:29.568180Z",
     "start_time": "2021-03-19T01:01:29.275990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 41.0])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 21.0]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "transformed_data2 = df2.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "transformed_df2 = spark.createDataFrame(transformed_data2, ['label', 'features'])\n",
    "transformed_df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:32.781346Z",
     "start_time": "2021-03-19T01:01:29.570469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is [('label', 'double'), ('features', 'vector'), ('features_scaled', 'vector')]\n",
      "test data is a [('label', 'double'), ('features', 'vector'), ('features_scaled', 'vector')]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler2 = standardScaler.fit(transformed_df2)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df2 = scaler2.transform(transformed_df2)\n",
    "\n",
    "seed = 314\n",
    "train_test = [0.8, 0.2]\n",
    "train_data2, test_data2 = scaled_df2.randomSplit(train_test,seed=seed)\n",
    "\n",
    "print(f'train data is {train_data2.dtypes}')\n",
    "print(f'test data is a {test_data2.dtypes}')\n",
    "\n",
    "linear_model2 = lr.fit(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:32.789362Z",
     "start_time": "2021-03-19T01:01:32.782087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bedrooms', 0.0),\n",
       " ('population', 0.0),\n",
       " ('households', 0.0),\n",
       " ('median_income', 0.27681502029465255),\n",
       " ('rooms_per_household', 0.0),\n",
       " ('housing_median_age', 0.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(df2.columns[1:],linear_model2.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:32.802291Z",
     "start_time": "2021-03-19T01:01:32.791687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000690432060719"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model2.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:32.836684Z",
     "start_time": "2021-03-19T01:01:32.803040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'features_scaled', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "predictions2 = linear_model2.transform(test_data2)\n",
    "print(predictions2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:33.341303Z",
     "start_time": "2021-03-19T01:01:32.837760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=2.1614311751602564, label=0.14999),\n",
       " Row(prediction=1.7411706113489145, label=0.225)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsandlabels_df2 = predictions2.select(\"prediction\", \"label\")\n",
    "predsandlabels_df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:34.304289Z",
     "start_time": "2021-03-19T01:01:33.343705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553845110003091"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE2 = predsandlabels_df2\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: (x[0] - x[1])**2)\\\n",
    "    .reduce(lambda x,y : x+y) /predsandlabels_df2.count()\n",
    "MSE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:34.506009Z",
     "start_time": "2021-03-19T01:01:34.305050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+------------------------+-------------------+------------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|median_house_value_final|rooms_per_household|population_per_household|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+------------------------+-------------------+------------------------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|                   4.526|  6.984126984126984|      2.5555555555555554|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|                   3.585|  6.238137082601054|       2.109841827768014|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+------------------------+-------------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code for Part 3 - Repeat Part 1 with at least one engineered feature based on one or more variables from the original set.\n",
    "df3 = df.withColumn('median_house_value_final', col('median_house_value')/100000)\n",
    "\n",
    "# add rooms_per_household \n",
    "df = df.withColumn('rooms_per_household', col('total_rooms')/col('households'))\n",
    "\n",
    "# add population_per_household (num people in the home)\n",
    "df3 = df.withColumn('population_per_household', col('population')/col('households'))\n",
    "\n",
    "\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:34.649019Z",
     "start_time": "2021-03-19T01:01:34.506726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------------+\n",
      "|median_house_value_final|total_bedrooms|population|households|median_income|rooms_per_household|population_per_household|\n",
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------------+\n",
      "|                   4.526|         129.0|     322.0|     126.0|       8.3252|  6.984126984126984|      2.5555555555555554|\n",
      "|                   3.585|        1106.0|    2401.0|    1138.0|       8.3014|  6.238137082601054|       2.109841827768014|\n",
      "+------------------------+--------------+----------+----------+-------------+-------------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retain these predictors for Part 1\n",
    "vars_to_keep2 = [\"median_house_value_final\", \n",
    "              \"total_bedrooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"median_income\",\n",
    "              \"rooms_per_household\",\n",
    "              \"population_per_household\"]\n",
    "\n",
    "# subset the dataframe on these predictors\n",
    "df3 = df3.select(vars_to_keep2)\n",
    "\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:35.061109Z",
     "start_time": "2021-03-19T01:01:34.649745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "transformed_data3 = df3.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "transformed_df3 = spark.createDataFrame(transformed_data3, ['label', 'features'])\n",
    "transformed_df3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:37.585004Z",
     "start_time": "2021-03-19T01:01:35.063378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is [('label', 'double'), ('features', 'vector'), ('features_scaled', 'vector')]\n",
      "test data is a [('label', 'double'), ('features', 'vector'), ('features_scaled', 'vector')]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "# Fit the DataFrame to the scaler; this computes the mean, standard deviation of each feature\n",
    "scaler3 = standardScaler.fit(transformed_df3)\n",
    "\n",
    "# Transform the data in `df2` with the scaler\n",
    "scaled_df3 = scaler3.transform(transformed_df3)\n",
    "\n",
    "seed = 314\n",
    "train_test = [0.8, 0.2]\n",
    "train_data3, test_data3 = scaled_df3.randomSplit(train_test,seed=seed)\n",
    "\n",
    "print(f'train data is {train_data3.dtypes}')\n",
    "print(f'test data is a {test_data3.dtypes}')\n",
    "\n",
    "linear_model3 = lr.fit(train_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:37.595314Z",
     "start_time": "2021-03-19T01:01:37.585799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bedrooms', 0.0),\n",
       " ('population', 0.0),\n",
       " ('households', 0.0),\n",
       " ('median_income', 0.276815072316851),\n",
       " ('rooms_per_household', 0.0),\n",
       " ('population_per_household', 0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(df3.columns[1:],linear_model3.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:37.605561Z",
     "start_time": "2021-03-19T01:01:37.597671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0006902309607646"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model3.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:37.641181Z",
     "start_time": "2021-03-19T01:01:37.606191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'features_scaled', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "predictions3 = linear_model3.transform(test_data3)\n",
    "print(predictions3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:38.103846Z",
     "start_time": "2021-03-19T01:01:37.643427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=2.1614311921997844, label=0.14999),\n",
       " Row(prediction=1.741170549408341, label=0.225)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsandlabels_df3 = predictions3.select(\"prediction\", \"label\")\n",
    "predsandlabels_df3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:38.973210Z",
     "start_time": "2021-03-19T01:01:38.106141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553844561430528"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE3 = predsandlabels_df3\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: (x[0] - x[1])**2)\\\n",
    "    .reduce(lambda x,y : x+y) /predsandlabels_df3.count()\n",
    "MSE3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:40.956601Z",
     "start_time": "2021-03-19T01:01:38.975521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code for Part 4\n",
    "\n",
    "# elasticNetParam corresponds to α and regParam corresponds to λ\n",
    "# Lasso - When λ>0 (i.e. regParam >0) and α = 1 (i.e. elasticNetParam =1), then the penalty is an L1 penalty.\n",
    "lr_lasso= LinearRegression(featuresCol=\"features\",\\\n",
    "                           labelCol=\"label\",\\\n",
    "                           predictionCol=\"prediction\",\\\n",
    "                           maxIter=maxIter,\\\n",
    "                           regParam=0.1,\\\n",
    "                           elasticNetParam=1.0)\n",
    "\n",
    "linear_model_lasso = lr_lasso.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:40.965603Z",
     "start_time": "2021-03-19T01:01:40.959265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('median_income', 0.0),\n",
       " ('housing_median_age', 0.0),\n",
       " ('total_rooms', 0.0),\n",
       " ('total_bedrooms', 0.36532274714313834),\n",
       " ('population', 0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(df.columns[1:],linear_model_lasso.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:40.976915Z",
     "start_time": "2021-03-19T01:01:40.967945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585499536995455"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model_lasso.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:41.009928Z",
     "start_time": "2021-03-19T01:01:40.979033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'features_scaled', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "predictions_lasso = linear_model_lasso.transform(test_data)\n",
    "print(predictions_lasso.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:41.412742Z",
     "start_time": "2021-03-19T01:01:41.010647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=2.190421297020153, label=0.14999),\n",
       " Row(prediction=1.6357883023074404, label=0.225)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsandlabels_lasso_df = predictions_lasso.select(\"prediction\", \"label\")\n",
    "predsandlabels_lasso_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:42.221001Z",
     "start_time": "2021-03-19T01:01:41.414975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691575957689366"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE4 = predsandlabels_lasso_df\\\n",
    "    .rdd\\\n",
    "    .map(lambda x: (x[0] - x[1])**2)\\\n",
    "    .reduce(lambda x,y : x+y) /predsandlabels_lasso_df.count()\n",
    "MSE4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE for Question part 1 to part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print dataframe containing `question_part`, `MSE` values for parts 1-4 in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:42.233414Z",
     "start_time": "2021-03-19T01:01:42.223320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question part</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prep data, train a model, predict, and evaluate model fit. Compute and repor...</td>\n",
       "      <td>0.755384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repeat Part 1 with at least one additional feature from the original set.</td>\n",
       "      <td>0.755385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Repeat Part 1 with at least one engineered feature based on one or more vari...</td>\n",
       "      <td>0.755384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Repeat Part 1 using Lasso Regression</td>\n",
       "      <td>0.691576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Question part  \\\n",
       "0  prep data, train a model, predict, and evaluate model fit. Compute and repor...   \n",
       "1        Repeat Part 1 with at least one additional feature from the original set.   \n",
       "2  Repeat Part 1 with at least one engineered feature based on one or more vari...   \n",
       "3                                             Repeat Part 1 using Lasso Regression   \n",
       "\n",
       "        MSE  \n",
       "0  0.755384  \n",
       "1  0.755385  \n",
       "2  0.755384  \n",
       "3  0.691576  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe containing question_part, MSE\n",
    "mse_dict = {\n",
    "        'Question part' :  ['prep data, train a model, predict, and evaluate model fit. Compute and report the Mean Squared Error (MSE)',\n",
    "                             'Repeat Part 1 with at least one additional feature from the original set.',\n",
    "                             'Repeat Part 1 with at least one engineered feature based on one or more variables from the original set.',\n",
    "                             'Repeat Part 1 using Lasso Regression'],\n",
    "                           \n",
    "         'MSE' : [MSE1, MSE2, MSE3, MSE4]\n",
    "        }\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"max_colwidth\", 80)\n",
    "mse_df = pd.DataFrame(mse_dict)\n",
    "mse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T01:01:47.170710Z",
     "start_time": "2021-03-19T01:01:42.235776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DS5559_M54HW__calhousing_JayHombal.ipynb to pdf\n",
      "[NbConvertApp] Writing 78507 bytes to DS5559_M54HW__calhousing_JayHombal.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert DS5559_M54HW__calhousing_JayHombal.ipynb --to pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit1d9fcce1e7154b63be1b4250f364c6c9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
