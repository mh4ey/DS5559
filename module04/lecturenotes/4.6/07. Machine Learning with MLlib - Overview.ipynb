{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib\n",
    "## *Introduction and Feature Extraction*\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "### Last Updated: June 11, 2020\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES \n",
    "\n",
    "1. Learning Spark\n",
    "2. Spark Documentation  \n",
    "\thttps://spark.apache.org/docs/latest/mllib-data-types.html  \n",
    "\thttp://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Introduction to the machine learning library\n",
    "2. Introduction to MLlib data types\n",
    "3. Discuss Feature Extraction tools in MLLib\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- pipeline  \n",
    "- supervised and unsupervised learning  \n",
    "- learning tasks: classification, regression, clustering, dimensionality reduction  \n",
    "- training set, testing set  \n",
    "- feature extraction  \n",
    "\n",
    "- MLlib data types:  \n",
    "  - LabeledPoint  \n",
    "  - sparse vector, dense vector  \n",
    "  - sparse matrix, dense matrix  \n",
    "  - Rating  \n",
    "\n",
    "- Feature Extraction  \n",
    "- TF-IDF  \n",
    "- Word2Vec  \n",
    "- Cosine Similarity  \n",
    "\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning in Spark**  \n",
    "Spark actually has two ML libraries: `MLlib` and `ML`, with `MLlib` being the older library; it is based on RDDs. The newer `ML` package is based on DataFrame use.  `ML` tends to be a more natural package, as users will generally have the data in DataFrames to build features.  This notebook covers `MLlib`, and `ML` will be covered later in the course.\n",
    "\n",
    "\n",
    "**MLlib**\n",
    "\n",
    "This is the original machine learning library from the \"olden\" days.\n",
    "It works on RDDs, because in the olden days, DataFrames did not yet exist, and people needed to do machine learning.\n",
    "\n",
    "MLlib contains only algorithms that can be parallelized, since those run well on clusters.  This does limit the algorithm choices.\n",
    "\n",
    "MLlib includes a pipeline API useful for building ML pipelines, similar to scikit-learn in Python.  It is HIGHLY recommended that you use pipelines.  They encapsulate the process, reducing the chance of errors, and making the scoring process simple.  More on pipelines later.\n",
    "\n",
    "Next, we jump right in, building a classifier and making predictions. You might not yet know about objects like `LabeledPoint`, but this should be fun and motivating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LogReg Classifier to Predict Spam vs Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES\n",
    "import os\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://udc-ba35-25c6:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mllib_classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd77971910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in spam and ham (not spam) data\n",
    "spam = sc.textFile(\"spam.txt\")\n",
    "ham = sc.textFile(\"ham.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...',\n",
       " 'Get Viagra real cheap!  Send money right away to ...',\n",
       " 'Oh my gosh you can be really strong too with these drugs found in the rainforest. Get them cheap right now ...',\n",
       " 'YOUR COMPUTER HAS BEEN INFECTED!  YOU MUST RESET YOUR PASSWORD.  Reply to this email with your password and SSN ...',\n",
       " 'THIS IS NOT A SCAM!  Send money and get access to awesome stuff really cheap and never have to ...']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note you wouldn't collect to driver if RDD was massive\n",
    "spam.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear Spark Learner, Thanks so much for attending the Spark Summit 2014!  Check out videos of talks from the summit at ...',\n",
       " 'Hi Mom, Apologies for being late about emailing and forgetting to send you the package.  I hope you and bro have been ...',\n",
       " 'Wow, hey Fred, just heard about the Spark petabyte sort.  I think we need to take time to try it out immediately ...',\n",
       " 'Hi Spark user list, This is my first question to this list, so thanks in advance for your help!  I tried running ...',\n",
       " \"Thanks Tom for your email.  I need to refer you to Alice for this one.  I haven't yet figured out that part either ...\",\n",
       " 'Good job yesterday!  I was attending your talk, and really enjoyed it.  I want to try out GraphX ...',\n",
       " 'Summit demo got whoops from audience!  Had to let you know. --Joe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a Term Frequency object using the hashing trick\n",
    "tf = HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the datasets, parsing on spaces\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build LabeledPoint datasets (1=spam, 0=ham)\n",
    "# LabeledPoints package (label, features) for each record\n",
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = positiveExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (10000,[0,365,1395,1451,1458,1819,2701,3834,4323,4671,5336,5469,5878,6300,6384,6910,7296,9101,9604],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = negativeExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, (10000,[0,1162,2403,2809,3080,3317,4161,4770,5423,5651,5743,5831,6006,6827,6971,7069,7872,9150,9370,9521,9604],[1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[6] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build training set; this stacks positive and negative records\n",
    "trainData = positiveExamples.union(negativeExamples)\n",
    "\n",
    "# cache since model training is recursive; o.w. would rebuild DataFrame\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LogReg model using default params\n",
    "model = LogisticRegressionWithSGD.train(trainData, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push \"not spam\" example through classifier. this is label=0\n",
    "Test = tf.transform(\"I love learning Spark programming\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for example: 0\n",
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "print(\"Prediction for example: {}\".format(model.predict(Test)))\n",
    "if model.predict(Test)==0:\n",
    "    print(\"CORRECT!\")\n",
    "else:\n",
    "    print(\"INCORRECT!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for example: 0\n",
      "INCORRECT!\n"
     ]
    }
   ],
   "source": [
    "# push \"not spam\" example through classifier. this is label=0\n",
    "Test2 = tf.transform(\"Get Viagra real cheap!\".split(\" \"))\n",
    "\n",
    "# Prediction\n",
    "print(\"Prediction for example: {}\".format(model.predict(Test2)))\n",
    "if model.predict(Test2)==1:\n",
    "    print(\"CORRECT!\")\n",
    "else:\n",
    "    print(\"INCORRECT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "Next we define the `MLlib` objects.\n",
    "\n",
    "**LabeledPoint**  \n",
    "Stores feature vector together with label  \n",
    "**Rating**  \n",
    "Rating of product by a user. Used in recommendation, for instance.  \n",
    "**Vector**  \n",
    "Handles dense and sparse. For sparse, only nonzero values and their indices are stored, along w vector length.  \n",
    "Sparse saves on memory and runtime.  \n",
    "**Matrix**  \n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single   machine.  \n",
    "MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order.  \n",
    "**Distributed matrix**  \n",
    "A distributed matrix has long-typed row and column indices and double-typed values  \n",
    "**Row matrix**  \n",
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices  \n",
    "**CoordinateMatrix**  \n",
    "CoordinateMatrix is a distributed matrix backed by an RDD of its entries  \n",
    "A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse.\n",
    "\n",
    "Take a look at this wiki to learn about row- versus column-major order.  It is super important to know how the data is saved.  Could you imagine what would happen to results if this were mixed up?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Row-_and_column-major_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse vector [1.0 0.0 2.0 0.0]\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "sv1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "*mllib.feature*  \n",
    "contains classes for common feature transformations:  \n",
    "-  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "Produces feature vectors from text documents\n",
    "\n",
    "There are two algorithms that compute TF-IDF:  \n",
    "\n",
    "**1. HashingTF**  \n",
    "\tComputes term frequency vector from document  \n",
    "\tCan process one document or an RDD of documents  \n",
    "\tEach document needs to be an interable sequence (a list in Python)  \n",
    "\n",
    "To reduce the chance of collision, we can increase the target feature dimension, i.e., the  \n",
    "\t number of buckets of the hash table. The default feature dimension is 1,048,576  \n",
    "\n",
    "**2. IDF**  \n",
    "\tComputes inverse document frequency  \n",
    "\tTerms that appear in high fraction of the docs are not as valuable  \n",
    "\tIDF will downweight such terms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a good example of Feature Extraction:  \n",
    "http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**  \n",
    "Computes distributed vector representation of words.  \n",
    "Similar words are close in the vector space  \n",
    "Useful in many NLP applications:  \n",
    "named entity recognition, disambiguation, parsing, tagging and machine translation.  \n",
    "\n",
    "The algorithm uses a neural network and some interesting concepts like the *hierarchical softmax*.  I encourage you to learn more if you have the time and interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Word2VecModel to some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "['Fed', 'expected', 'to', 'leave', 'interest', 'rates', 'alone', 'as', 'Trump', 'pushes', 'for', 'more', 'cuts']\n",
      "['Donna', 'Borak', 'byline']\n",
      "['By', 'Donna', 'Borak,', 'CNN', 'Business']\n",
      "['']\n",
      "['Trump', 'asks', 'if', 'Federal...']\n",
      "-----------------\n",
      "the: 0.2611371576786041\n",
      "Powell: 0.2211126983165741\n",
      "economy: 0.20416149497032166\n",
      "policy: 0.14639779925346375\n",
      "of: 0.1154128685593605\n",
      "and: 0.11317558586597443\n",
      "rates: 0.10572120547294617\n",
      "that: 0.10568731278181076\n",
      "to: 0.09924145042896271\n",
      "is: 0.09905049204826355\n",
      "US: 0.09054430574178696\n",
      "have: 0.06850812584161758\n",
      "has: 0.04858196899294853\n",
      "a: 0.02839130535721779\n",
      "Fed: 0.01105659082531929\n",
      "be: 0.0006084572523832321\n",
      "said: -0.0003059235750697553\n",
      "for: -0.025064580142498016\n",
      "Trump: -0.03947291150689125\n",
      "if: -0.08069650828838348\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = sc.textFile(\"fed_rates_article.txt\").map(lambda row: row.split(\" \"))\n",
    "topk = 5\n",
    "print('First {} records:'.format(topk))\n",
    "first_five = inp.take(topk)\n",
    "for i in range(topk):\n",
    "    print(first_five[i])\n",
    "print(\"-----------------\")\n",
    "                        \n",
    "word2vec = Word2Vec() # construct Word2Vec object\n",
    "model = word2vec.fit(inp) # train Word2Vec on the dasta\n",
    "\n",
    "# apply Word2Vec to find synonyms by representing words as vectors\n",
    "synonyms = model.findSynonyms('rate', 20)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler**   \n",
    "\n",
    "Standardization can improve the convergence rate during the optimization process, and it also prevents against features with very large variances exerting an overly large influence during model   training.  \n",
    "\n",
    "For each feature,  \n",
    "1. Scales to unit variance  \n",
    "2. Centers to mean zero  \n",
    "Useful or even essential for some models  \n",
    "\n",
    "`K-means` works in Euclidean space, so all features should be on same scale  \n",
    "\n",
    "Tree models do not need this\n",
    "\n",
    "Use this in a *Pipeline* so the statistics can be applied to datasets for scoring later. You would NOT compute means and standard deviations on the scoring set to standardize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler  \n",
    "Load dataset in libsvm format, standardize the features so that the new features have unit variance and/or zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Print the label and features (before scaling) from the first record in *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute the first 20 synonyms of the word \"economy.\" Then extract and print the cosine distances.  Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Copy the Ham/Spam classifier code in the cell below.  Then try a different model, leaving the rest of the code unchanged.  Run the code.  Does it get the \"not spam\" example right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
